<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Online AUC Maximization | Haji blog</title>
    <link rel="stylesheet" href="https://haaforever.github.io/blog/css/style.css" />
    <link rel="stylesheet" href="https://haaforever.github.io/blog/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="https://haaforever.github.io/blog/">Home</a></li>
      
      <li><a href="https://haaforever.github.io/blog/about/">About</a></li>
      
      <li><a href="https://haaforever.github.io/blog/categories/">Categories</a></li>
      
      <li><a href="https://haaforever.github.io/blog/tags/">Tags</a></li>
      
      <li><a href="https://haaforever.github.io/blog/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Online AUC Maximization</span></h1>

<h2 class="date">2020/12/07</h2>
</div>

<main>

<link href="https://haaforever.github.io/blog/post/2020/12/07/online-auc-maximization/index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="https://haaforever.github.io/blog/post/2020/12/07/online-auc-maximization/index_files/anchor-sections/anchor-sections.js"></script>



<p>Zhao et al., 2011, <strong>Online AUC Maximization</strong>, <em>ICML</em>. <a href="https://scholar.google.co.kr/scholar?cluster=3998144449274225696&amp;hl=ko&amp;as_sdt=0,5">pdf</a></p>
<hr />
<div id="motivation" class="section level2">
<h2>Motivation</h2>
<ul>
<li>Most studies of online learning measure the performance of a learner by classification accuracy, which is inappropriate for applications where the data are unevenly distributed among different classes</li>
<li>AUC maximization needs to optimize the pairwise loss between two instances from different classes, making it unattractive for large-scale applications</li>
<li>The authors address this challenge by exploiting the reservoir sampling</li>
</ul>
<hr />
</div>
<div id="preliminaries" class="section level2">
<h2>Preliminaries</h2>
<ul>
<li><span class="math inline">\((\mathbf{x}_t, y_t)\)</span>: the training examples received at the <span class="math inline">\(t\)</span>-th trial where <span class="math inline">\(\mathbf{x}_t \in \mathbb{R}^d\)</span> and <span class="math inline">\(y_t \in \{ -1, 1\}\)</span></li>
<li>Training dataset: <span class="math inline">\(\mathcal{D} = \{ (\mathbf{x}_i, y_i) \in \mathbb{R} \times \{ -1, 1\} | i \in [T] \}\)</span></li>
<li>The set of positive instances: <span class="math inline">\(\mathcal{D}_+ = \{ (\mathbf{x}_i^+, 1) | i \in [T_+] \}\)</span></li>
<li>The set of negative instances: <span class="math inline">\(\mathcal{D}_- = \{ (\mathbf{x}_j^-, -1) | j \in [T_-] \}\)</span></li>
<li><span class="math inline">\(T_+\)</span>, <span class="math inline">\(T_-\)</span>: the number of positive and negative instances</li>
<li>Linear classifier: <span class="math inline">\(\mathbf{w} \cdot \mathbf{x}\)</span></li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
\text{AUC} (\mathbf{w}) &amp; = \frac{\sum_{i=1}^{T^+} \sum_{j=1}^{T_-} \mathbb{I} (\mathbf{w} \cdot \mathbf{x}_i^+ &gt; \mathbf{w} \cdot \mathbf{x}_j^-)}{T_+ T_-} \\[10pt]
&amp; = 1 - \frac{\sum_{i=1}^{T^+} \sum_{j=1}^{T_-} \mathbb{I} (\mathbf{w} \cdot \mathbf{x}_i^+ \leq \mathbf{w} \cdot \mathbf{x}_j^-)}{T_+ T_-}
\end{split}
\end{equation}\]</span></p>
<ul>
<li>Using hinge loss <span class="math inline">\(l(\mathbf{w}, \mathbf{x} - \mathbf{x}&#39;)\)</span>, optimal classifier by minimizing the following objective</li>
</ul>
<p><span class="math display">\[\begin{equation}
\frac{1}{2} \lVert \mathbf{w} \rVert _2^2 + C \sum_{i=1}^{T_+} \sum_{j=1}^{T_-} \max \{ 0, 1 - \mathbf{w} \cdot (\mathbf{x}_i^+ - \mathbf{x}_j^-) \}
\end{equation}\]</span></p>
<hr />
</div>
<div id="settings-for-online-learning" class="section level2">
<h2>Settings for online learning</h2>
<ul>
<li>Re-writing the pairwise summation into a sum of losses for individual instances</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp; L_+^t (\mathbf{w}) = \sum_{k=1}^{t-1} \mathbb{I}(y_{k} = -1) l(\mathbf{w}, \mathbf{x}_t - \mathbf{x}_{k}) \\[10pt]
&amp; L_-^t (\mathbf{w}) = \sum_{k=1}^{t-1} \mathbb{I}(y_{k} = 1) l(\mathbf{w}, \mathbf{x}_t - \mathbf{x}_{k}) \\[10pt]
&amp; L_t (\mathbf{w}) = \mathbb{I}(y_t = 1) L_+^t (\mathbf{w}) + \mathbb{I}(y_t = -1) L_-^t (\mathbf{w}) \\[10pt]
&amp; \frac{1}{2} \lVert \mathbf{w} \rVert _2^2 + C \sum_{t=1}^T L_t (\mathbf{w})
\end{split}
\end{equation}\]</span></p>
<ul>
<li>Directly applying the gradient descent based online learning algorithm (Zinkevich and Martin, 2003) needs to store all the received training examples, making it impractical for large-scale online learning tasks</li>
<li>Caching a small number of received training examples by introducing two buffers</li>
<li>Two buffers <span class="math inline">\(B_+\)</span> and <span class="math inline">\(B_-\)</span> of size <span class="math inline">\(N_+\)</span> and <span class="math inline">\(N_-\)</span></li>
<li>For example <span class="math inline">\((\mathbf{x}_t, y_t)\)</span> received at trail <span class="math inline">\(t\)</span>, first update the two buffers, and then update the linear classifier using two buffers</li>
<li><span class="math inline">\(\mathbb{E}[ \cdot ]\)</span>: expectation over the randomly sample instances in buffers</li>
</ul>
<p><br>
<img src="images/Algorithm_01.PNG" />
<br></p>
<hr />
</div>
<div id="update-buffer" class="section level2">
<h2>Update Buffer</h2>
<ul>
<li>Maintaining an accurate sketch of history under the constraint of fixed buffer size</li>
<li>Reservoir sampling (Vitter and Scott, 1985)
<ul>
<li>Given <span class="math inline">\((\mathbf{x}_t, y_t)\)</span></li>
<li>Add it to the buffer <span class="math inline">\(B_{y_t}^t\)</span> if <span class="math inline">\(|B_{y_t}^t| &lt; N_{y_t}\)</span></li>
<li>Otherwise, with probability <span class="math inline">\(N_{y_t} / N_{y_t}^{t+1}\)</span>, update the buffer <span class="math inline">\(B_{y_t}^t\)</span> by randomly replacing one instance in <span class="math inline">\(B_{y_t}^t\)</span> with <span class="math inline">\(\mathbf{x}_t\)</span></li>
</ul></li>
<li>The instances in the buffers simulate an uniform sampling from the original dataset</li>
<li><strong>Lemma 1.</strong> For any function <span class="math inline">\(f: \mathbb{R}^d \to \mathbb{R}\)</span> and at any iteration <span class="math inline">\(t\)</span>, we have</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp; \frac{1}{|B_+^t|} \mathbb{E} \left[ \sum_{\mathbf{x} \in B_+^t} f(\mathbf{x}) \right] = \frac{1}{N_+^t} \sum_{i=1}^{t} \mathbb{I} (y_t = 1) f(\mathbf{x}_i) \\[10pt]
&amp; \frac{1}{|B_-^t|} \mathbb{E} \left[ \sum_{\mathbf{x} \in B_-^t} f(\mathbf{x}) \right] = \frac{1}{N_-^t} \sum_{i=1}^{t} \mathbb{I} (y_t = -1) f(\mathbf{x}_i)
\end{split}
\end{equation}\]</span></p>
<hr />
</div>
<div id="update-classifier" class="section level2">
<h2>Update Classifier</h2>
<div id="sequential-updating" class="section level4">
<h4>Sequential Updating</h4>
<ul>
<li>Treating <span class="math inline">\(\{ (\mathbf{x}_t, \mathbf{x}), \mathbf{x} \in B \}\)</span> as a sequence of pairwise instances</li>
<li>Applying an online learning algorithm to update <span class="math inline">\(\mathbf{w}\)</span> w.r.t. the sequence of pairwise instances</li>
<li><strong>Theorem 1.</strong> After running the Algorithm 1 with the sequential updating in Algorithm 3 and reservoir sampling, for any <span class="math inline">\(\mathbf{w}\)</span>, we have the following where <span class="math inline">\(T_+\)</span> and <span class="math inline">\(T_-\)</span> are the total number of positive and negative instances received over <span class="math inline">\(T\)</span> trials.</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mathbb{E} \left[ \sum_{t=1}^T L_t (\mathbf{w}) \right] \leq \sum_{t=1}^T L_t (\mathbf{w}) + \frac{\lVert \mathbf{w} \rVert _2^2}{C} + \frac{C}{3} (N_+ T_+^3 + N_- T_-^3)
\end{equation}\]</span></p>
<ul>
<li>For the optimization problem (6), its closed-form solution is given by <span class="math inline">\(\mathbf{w}^{i+1} = \mathbf{w}^i + \tau y_t (\mathbf{x}_t - \mathbf{x})\)</span> where <span class="math inline">\(\tau\)</span> can be computed by</li>
</ul>
<p><span class="math display">\[\begin{equation}
\tau = \min{ \left\{ \frac{C_t}{2}, \frac{l(\mathbf{w}^i, y_t (\mathbf{x}_t - \mathbf{x}))}{\lVert \mathbf{x}_t - \mathbf{x} \rVert _2^2} \right\} }
\end{equation}\]</span></p>
<p><br>
<img src="images/Algirithm_03.PNG" />
<br></p>
</div>
<div id="gradient-updating" class="section level4">
<h4>Gradient Updating</h4>
<p><br>
<img src="images/Algorithm_04.PNG" />
<br></p>
<blockquote>
<p>Error in Algorith 3, 4: not <span class="math inline">\(\text{for } \mathbf{x} \in B\)</span>, but <span class="math inline">\(\text{for } \mathbf{x} \in B_{-y_t}\)</span></p>
</blockquote>
<hr />
</div>
</div>
<div id="experiments" class="section level2">
<h2>Experiments</h2>
<ul>
<li>12 datasets from LIBSVM website and UCI ML repository</li>
<li>5-fold cross validation with 4 independent repetitions</li>
<li><span class="math inline">\(N_- = N_+ = 100\)</span> and <span class="math inline">\(C \in 2^{[-10:10]}\)</span></li>
<li>Compared methods
<ul>
<li><span class="math inline">\(\text{Perceptron}\)</span>: Rosenblatt’s perceptron</li>
<li><span class="math inline">\(\text{PA}\)</span>: Passive-Aggressive algorithm (Crammer et al., 2006)</li>
<li><span class="math inline">\(\text{CW-full}\)</span>: confidence-based weighted online learning algorithm (Crammer et al., 2008)</li>
<li><span class="math inline">\(\text{CPA}_\text{PB}\)</span>: Prediction-Based Cost-sensitive Passive-Aggressive algorithm (Crammer et al., 2006)</li>
<li><span class="math inline">\(\text{CPA}_\text{ML}\)</span>: Max-Loss Cost-sensitive Passive-Aggressive algorithm (Crammer et al., 2006)</li>
<li><span class="math inline">\(\text{OAM}_\text{seq}\)</span>: OAM with sequential updating</li>
<li><span class="math inline">\(\text{OAM}_\text{gra}\)</span>: OAM with gradient updating</li>
<li><span class="math inline">\(\text{OAM}_\text{inf}\)</span>: OAM with infinite buffer size</li>
</ul></li>
</ul>
<p><br>
<img src="images/Table_01.PNG" />
<br>
<img src="images/Table_02.PNG" />
<br>
<img src="images/Fig_01.PNG" />
<br></p>
<hr />
</div>
<div id="further-study" class="section level2">
<h2>Further Study</h2>
<ul>
<li>Cortes and Mohri, 2004, <strong>AUC optimization vs. error rate minimization</strong>, <em>Advances in Neural Information Processing Systems</em>. <a href="https://scholar.google.co.kr/scholar?cluster=3571958290589699987&amp;hl=ko&amp;as_sdt=0,5">pdf</a></li>
<li>Joachims, 2005. <strong>A support vector method for multivariate performance measures</strong>, <em>International Conference on Machine Learning</em>. <a href="https://scholar.google.co.kr/scholar?cluster=14930548401865894598&amp;hl=ko&amp;as_sdt=0,5">pdf</a></li>
</ul>
<hr />
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li>Crammer et al., 2006, <strong>Online passive-aggressive algorithms</strong>, <em>Journal of Machine Learning Research</em>. <a href="https://www.jmlr.org/papers/v7/crammer06a">pdf</a></li>
<li>Crammer et al., 2008, <strong>Exact convex confidence-weighted learning</strong>, <em>Advances in Neural Information Processing Systems</em>. <a href="https://proceedings.neurips.cc/paper/2008/hash/68ce199ec2c5517597ce0a4d89620f55-Abstract.html">pdf</a></li>
<li>Vitter and Scott, 1985, <strong>Random sampling with a reservoir</strong>, <em>ACM Transactions on Mathematical Software</em>. <a href="https://scholar.google.co.kr/scholar?cluster=12477259931641459543&amp;hl=ko&amp;as_sdt=0,5">pdf</a></li>
<li>Zinkevich and Martin, 2003, <strong>Online convex programming and generalized infinitesimal gradient ascent</strong>, <em>International Conference on Machine Learning</em>. <a href="https://www.aaai.org/Papers/ICML/2003/ICML03-120.pdf">pdf</a></li>
</ul>
<hr />
</div>
<div id="note" class="section level2">
<h2>Note</h2>
<ul>
<li>Kernel formulation for nonlinear classification</li>
<li>Extension to other loss functions using <span class="math inline">\(L_t(\mathbf{w})\)</span> formulation</li>
</ul>
</div>

</main>

  <footer>
  <script src="//yihui.org/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.org/js/center-img.js"></script>

  
  <hr/>
  © <a href="https://www.instagram.com/hajiboost/">Jihyeon Ha</a> 2020 &ndash; | <a href="https://github.com/haaforever">Github</a>
  
  </footer>
  </body>
</html>

