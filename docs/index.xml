<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Haji blog</title>
    <link>https://haaforever.github.io/blog/</link>
    <description>Recent content in Home on Haji blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://haaforever.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fast Stochastic AUC Maximization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/10/fast-stochastic-auc-maximization/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/10/fast-stochastic-auc-maximization/</guid>
      <description>Liu et al., 2016, Fast stochastic AUC maximization with \(O(1/n)\) convergence rate, ICML. pdf
IntroductionConvergence rate of SOLAM (Ying et al., 2016): \(\tilde{\mathcal{O}} (1 / \sqrt{n})\)Need to improve the convergence rate of stochastic optimizationThis paper proposes FSAUC algorithm with convergence rate of \(\tilde{\mathcal{O}} (1 / n)\)BackgroundInput space \(\mathcal{X} \subseteq \mathbb{R}^d\), output space \(\mathcal{Y} = \{ -1, 1\}\)Training data \(\mathbf{z} = \{ (\mathbf{x}_i, y_i) \}_{i = 1}^n\): i.</description>
    </item>
    
    <item>
      <title>Stochastic Online AUC Maximization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/08/stochastic-online-auc-maximization/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/08/stochastic-online-auc-maximization/</guid>
      <description>Ying et al., 2016, Stochastic online AUC maximization, NIPS. pdf
IntroductionTopic: online learning algorithms that maximize AUC for large-scale dataChallenge: learning objective is usually defined over a pair of training examples of opposite classesProposed methodAUC optimization can be equivalently formulated as a convex-concave saddle point problem (SPP)From this SPP representation, a stochastic online algorithm (SOLAM) is proposed which has time and space complexity of one datumBackgroundInput space \(\mathcal{X} \subseteq \mathbb{R}^d\)Output space \(\mathcal{Y} = \{ -1, 1\}\)Training data \(\mathbf{z} = \{ (x_i, y_i) \}_{i = 1}^n\): i.</description>
    </item>
    
    <item>
      <title>AUC Optimization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/auc-optimization/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/auc-optimization/</guid>
      <description>ApproximationYan et al., 2003, Optimizing classifier performance via an approximation to the Wilcoxon-Mann-Whitney statistic, International Conference on Machine Learning. pdf, summarySteck, 2007, Hinge rank loss and the area under the ROC curve, European Conference on Machine Learning. pdf, summaryCalders and Jaroszewicz, 2007, Efficient AUC optimization for classification, European Conference on Principles of Data Mining and Knowledge Discovery. pdfAnalysisCortes and Mohri, 2004, AUC optimization vs.</description>
    </item>
    
    <item>
      <title>Hinge Rank Loss and AUC</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/hinge-rank-loss-and-auc/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/hinge-rank-loss-and-auc/</guid>
      <description>Steck, 2007, Hinge rank loss and the are under the ROC curve, European Conference on Machine Learning. pdf
NotationDataset\(D = \{ (x_i, y_i) \} _{i=1}^{N}\)Class labels \(y_i \in \{ -1, +1 \}\)The number of positive and negative examples \(N^+\) and \(N^-\) respectivelyClassifier \(C\)Real-valued output \(c_i = C(x_i)\)Assume that there are no ties, i.e., \(c_i \neq c_j\) for all \(i, j\)Classification rule given the real-valued threshold \(\theta\): \(sign(c_i - \theta)\)Rank-version of \(C\)The smallest output-value gets assigned the lowest rankLet \(r_i \in \{ 1, \cdots, N \}\) be the rank of \(x_i\)\(r_j^+\): ranks of the positive examples, \(j = 1, \cdots, N^+\)\(r_k^-\): ranks of the negative examples, \(k = 1, \cdots, N^-\)Rank-threshold \(\tilde{\theta} = \max{\{ r_i: c_i \leq \theta \}} + 0.</description>
    </item>
    
    <item>
      <title>Label Distribution Learning</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/label-distribution-learning/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/label-distribution-learning/</guid>
      <description>Geng, 2016, Label distribution learning, IEEE Transactions on Knowledge and Data Engineering. pdf
Formulation of LDLNotationInput space: \(\mathbf{x} \in \mathcal{X} = \mathbb{R}^q\)Complete set of labels: \(\mathcal{Y} = \{ y_1, \cdots, y_c \}\)Label distribution of \(\mathbf{x}_i\): \(D_i = \{ d_{\mathbf{x}_i}^{y_1}, \cdots, d_{\mathbf{x}_i}^{y_c} \}\)Description degree of \(y\) to \(\mathbf{x}\): \(d_{\mathbf{x}}^y\)\(d_{\mathbf{x}}^y \in [0,1]\) and \(\sum_y d_{\mathbf{x}}^y = 1\)Training set: \(S = \{ (\mathbf{x}_1, D_1), \cdots, (\mathbf{x}_n, D_n) \}\)Learn a conditional probability mass function \(p(y|\mathbf{x})\) from \(S\), where \(\mathbf{x} \in \mathcal{X}\) and \(y \in \mathcal{Y}\)Optimization formulationGiven a parametric model \(p(y|\mathbf{x} ; \theta)\), the goal of LDL is to find \(\theta\) that can generate a distribution similar to \(D_i\), given \(\mathbf{x}_i\)Example: Kullback-Leibler divergence\[\begin{equation}\begin{split}&amp;amp; \mathop{\mathrm{argmin}}_{\theta} \sum_i \sum_j \left( d_{\mathbf{x}_i}^{y_j} \log{\frac{d_{\mathbf{x}_i}^{y_j}}{p(y_j|\mathbf{x}_i ; \theta)}} \right) \\= \quad &amp;amp; \mathop{\mathrm{argmax}}_{\theta} \sum_i \sum_j d_{\mathbf{x}_i}^{y_j} \log{p(y_j|\mathbf{x}_i ; \theta)}\end{split}\end{equation}\]When \(d_{\mathbf{x}_i}^{y_j} = I(y_j = y(\mathbf{x}_i))\), single label learning\[\begin{equation}\mathop{\mathrm{argmax}}_{\theta} \sum_i \log{p(y(\mathbf{x}_i)|\mathbf{x}_i ; \theta)}\end{equation}\]When each instance is associated with a specific label set, multi-label learning\[\begin{equation}\mathop{\mathrm{argmax}}_{\theta} \sum_i \frac{1}{|Y_i|} \sum_{y \in Y_i} \log{p(y | \mathbf{x}_i ; \theta)}\end{equation}\]LDL may be viewed as a more general learning framework which includes both SLL and MLL as its special cases</description>
    </item>
    
    <item>
      <title>nloptr</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/nloptr/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/nloptr/</guid>
      <description>IntroductionArguments of nloptr(â€¦)OptionsMinimizing the Rosenbrock Banana FunctionMinimization with Inequality ConstraintsProjection onto Convex SetSummary of vignettes
IntroductionR interface to NLopt (free/open-source library)NLopt addresses general nonlinear optimization problems of the form\[\begin{equation}\begin{split}&amp;amp; \min_{x \in \mathbb{R}^n} f(x) \\[10pt]\text{subject to} \quad &amp;amp; g(x) \leq 0 \\[10pt]&amp;amp; h(x) = 0 \\[10pt]&amp;amp; x_L \leq x \leq x_U\end{split}\end{equation}\]</description>
    </item>
    
    <item>
      <title>Noise Constrative Estimation</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/noise-constrative-estimation/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/noise-constrative-estimation/</guid>
      <description>Gutmann and Hyvarinen, 2010, Noise-contrastive estimation: A new estimation principle for unnormalized statistical models, AISTATS. pdf
Estimation ProblemGiven a sample of a random vector \(\mathbf{x} \in \mathbb{R}^n\) from an unknown pdf \(p_d(\cdot)\)Our model: a parameterized family of functions \(\{ p_m(\cdot;\alpha) \}_{\alpha}\)How to estimate \(\alpha\) from the observed sample by maximizing some objective function?Any solution to this estimation problem must yield a normalized density\[\begin{equation}\begin{split}&amp;amp; p_m(\cdot;\alpha) = \frac{p_m^0(\cdot;\alpha)}{Z(\alpha)} \\[10pt]&amp;amp; Z(\alpha) = \int p_m^0(\mathbf{u};\alpha) d\mathbf{u}\end{split}\end{equation}\]</description>
    </item>
    
    <item>
      <title>One-Pass AUC Optimization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/one-pass-auc-optimization/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/one-pass-auc-optimization/</guid>
      <description>Gao et al., 2013, One-pass AUC optimization, ICML. pdf
MotivationAUC is measured by a sum of losses defined over pairs of instances from different classesThis causes a problem in applications involving big data or streaming data in which a large volume of data come in a short time periodIn this work, one-pass AUC optimization that requires going through the training data only once without storing the entire training dataset is proposedPreliminariesInstance space \(\mathcal{X} \in \mathbb{R}^d\)Label set \(\mathcal{Y} = \{ -1, +1 \}\)Unknown distribution \(\mathcal{D}\) over \(\mathcal{X} \times \mathcal{Y}\)A training sample of \(n_{+}\) positive instances and \(n_{-}\) negative onesTraining set \(\mathcal{S} = \{ (\mathbf{x}_1^{+}, +1), \cdots, (\mathbf{x}_{n_{+}}^{+}, +1), (\mathbf{x}_1^{-}, -1), \cdots, (\mathbf{x}_{n_{-}}^{-}, -1) \}\)Real-valued function \(f: \mathcal{X} \rightarrow \mathbb{R}\)AUC of \(f\) on \(\mathcal{S}\)\[\begin{equation}\sum_{i=1}^{n_{+}} \sum_{j=1}^{n_{-}} \frac{\mathbb{I}[f(\mathbf{x}_{i}^{+}) &amp;gt; f(\mathbf{x}_{j}^{-})]}{n_{+} n_{-}}\end{equation}\]</description>
    </item>
    
    <item>
      <title>Online AUC Maximization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/online-auc-maximization/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/online-auc-maximization/</guid>
      <description>Zhao et al., 2011, Online AUC Maximization, ICML. pdf
MotivationMost studies of online learning measure the performance of a learner by classification accuracy, which is inappropriate for applications where the data are unevenly distributed among different classesAUC maximization needs to optimize the pairwise loss between two instances from different classes, making it unattractive for large-scale applicationsThe authors address this challenge by exploiting the reservoir samplingPreliminaries\((\mathbf{x}_t, y_t)\): the training examples received at the \(t\)-th trial where \(\mathbf{x}_t \in \mathbb{R}^d\) and \(y_t \in \{ -1, 1\}\)Training dataset: \(\mathcal{D} = \{ (\mathbf{x}_i, y_i) \in \mathbb{R} \times \{ -1, 1\} | i \in [T] \}\)The set of positive instances: \(\mathcal{D}_+ = \{ (\mathbf{x}_i^+, 1) | i \in [T_+] \}\)The set of negative instances: \(\mathcal{D}_- = \{ (\mathbf{x}_j^-, -1) | j \in [T_-] \}\)\(T_+\), \(T_-\): the number of positive and negative instancesLinear classifier: \(\mathbf{w} \cdot \mathbf{x}\)\[\begin{equation}\begin{split}\text{AUC} (\mathbf{w}) &amp;amp; = \frac{\sum_{i=1}^{T^+} \sum_{j=1}^{T_-} \mathbb{I} (\mathbf{w} \cdot \mathbf{x}_i^+ &amp;gt; \mathbf{w} \cdot \mathbf{x}_j^-)}{T_+ T_-} \\[10pt]&amp;amp; = 1 - \frac{\sum_{i=1}^{T^+} \sum_{j=1}^{T_-} \mathbb{I} (\mathbf{w} \cdot \mathbf{x}_i^+ \leq \mathbf{w} \cdot \mathbf{x}_j^-)}{T_+ T_-}\end{split}\end{equation}\]</description>
    </item>
    
    <item>
      <title>Optimizing AUC Using Wilcoxon-Mann-Whitney Statistic</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/optimizing-auc-using-wilcoxon-mann-whitney-statistic/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/optimizing-auc-using-wilcoxon-mann-whitney-statistic/</guid>
      <description>Yan et al., 2003, Optimizing classifier performance via an approximation to the Wilcoxon-Mann-Whitney statistic, International Conference on Machine Learning. pdf
IntroductionRelationship between AUC and WMW statisticMinimizing cross entropy or mean squared error does not necessarily maximize the area under the ROC curve (AUC)Let \(\{ x_0 , \cdots , x_{m-1} \}\) as the classifier outputs for \(m\) positive examples, and \(\{ y_0 , \cdots , y_{n-1} \}\) for \(n\) negative examples\(X\) and \(Y\) are random variableWilcoxon-Mann-Whitney (WMW) statistic \(U\) is an estimator of AUC \(\textit{P} (X &amp;gt; Y)\)\[U = \frac{ \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} I(x_i , y_j)}{mn}\]</description>
    </item>
    
    <item>
      <title>Some Experiments on Loss Functions</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/some-experiments-on-loss-functions/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/some-experiments-on-loss-functions/</guid>
      <description>Still working
Surrogate loss functions\(y \in \{ -1, 1 \}\), \(h(x) \in \mathbb{R}\)Classification rule\[\begin{equation*}\hat{y} = \begin{cases}1 &amp;amp; h(x) \geq 0 \\-1 &amp;amp; \text{otherwise} \end{cases}\end{equation*}\]
Classification margin \(m = yh(x) \in \mathbb{R}\)Margin-based loss function \(L(y, h(x))\)LossFormula0-1\(I\{yh(x)&amp;lt;0\}\)Logistic\(\log _2 (1+\exp(-yh(x)))\)Exponential\(\exp(-yh(x))\)Hinge\(\max \{ 0, 1-yh(x) \} = \{ 1-yh(x) \}_{+}\)Square\((1-yh(x))^2\)Savage\(1/(1+\exp(yh(x)))^2\)Sigmoid\(1/(1+\exp(yh(x)))\)Binomial deviance\(\log _2 (1 + \exp(-2yh(x)))\)Tangent\((2 \tan ^{-1} (yh(x))- 1)^2\)Data descriptionTwo 2D Gaussian distributionPositive samples \(X^{+} \sim N(\mu^{+}, I_2)\) where \(\mu^{+} = (1,1)^T\)Negative samples \(X^{-} \sim N(\mu^{-}, I_2)\) where \(\mu^{+} = (0,0)^T\)\(I_2\): \((2 \times 2)\) identity matrixSame prior distributionExperiments on error minimizationFind the optimal linear classifier \(h^{\star}\)Linear classifier \(h(\mathbf{x}) = w_{0} + w_1x_1 + w_2x_2 = w_{0} + \mathbf{w} \cdot \mathbf{x}\)Given training dataset \(D = \{ (\mathbf{x}_i,y_i) \}_{i=1}^{n} = \{ (\mathbf{x}^{+}_{1},1), \cdots, (\mathbf{x}^{+}_{n^{+}},1), (\mathbf{x}^{-}_{1},-1), \cdots, (\mathbf{x}^{-}_{n^{-}},-1) \}\)Original formulation\[\mathop{\mathrm{argmin}}_{w_{0}, \mathbf{w}} \Pr (Yh(X)&amp;lt;0)\]</description>
    </item>
    
    <item>
      <title>Jihyeon Ha</title>
      <link>https://haaforever.github.io/blog/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/about/</guid>
      <description>Data scientist with a strong background in machine learning, statistics, optimization and passion Contact haaforever@naver.com
Education  Technical Research Personnel as a Korean Military Service, mobiis (2019-2021) Ph.D. program in Industrial Engineering, Sungkyunkwan University (2015-present) M.S. in Industrial Engineering, Sungkyunkwan University (2013-2014) B.S. in Industrial Engineering, Sungkyunkwan University (2009-2012)  Journal  Kim, T., Ha, J., Lee, J. S., Oh, Y., &amp;amp; Cho, Y. J. (2016). Discovering relationships between skin type and life style using data mining techniques: a case study of korea.</description>
    </item>
    
  </channel>
</rss>
