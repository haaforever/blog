<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Online Learning on Haji blog</title>
    <link>https://haaforever.github.io/blog/tags/online-learning/</link>
    <description>Recent content in Online Learning on Haji blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://haaforever.github.io/blog/tags/online-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SPAM - Stochastic Proximal Algorithms for AUC Maximization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/29/spam-stochastic-proximal-algorithms-for-auc-maximization/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/29/spam-stochastic-proximal-algorithms-for-auc-maximization/</guid>
      <description>Natole et al., 2018, Stochastic proximal algorithms for AUC maximization, International Conference on Machine Learning. pdf
IntroductionStochastic proximal algorithm for AUC maximization (SPAM)SPAM applies to general non-smooth regularization termsUnder the assumption of strong convexity, SPAM can achieve a convergence rate of \(\mathcal{O}(\frac{\log{t}}{t})\)BackgroundInput space \(\mathcal{X} \subseteq \mathbb{R}^d\)Output space \(\mathcal{Y} = \{ -1, 1\}\)Training data \(\mathbf{z} = \{ (x_i, y_i) \}_{i = 1}^n\): i.</description>
    </item>
    
    <item>
      <title>FSAUC - Fast Stochastic AUC Maximization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/10/fast-stochastic-auc-maximization/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/10/fast-stochastic-auc-maximization/</guid>
      <description>Liu et al., 2016, Fast stochastic AUC maximization with \(O(1/n)\) convergence rate, ICML. pdf
IntroductionConvergence rate of SOLAM (Ying et al., 2016): \(\tilde{\mathcal{O}} (1 / \sqrt{n})\)Need to improve the convergence rate of stochastic optimizationThis paper proposes FSAUC algorithm with convergence rate of \(\tilde{\mathcal{O}} (1 / n)\)BackgroundInput space \(\mathcal{X} \subseteq \mathbb{R}^d\), output space \(\mathcal{Y} = \{ -1, 1\}\)Training data \(\mathbf{z} = \{ (\mathbf{x}_i, y_i) \}_{i = 1}^n\): i.</description>
    </item>
    
    <item>
      <title>SOLAM - Stochastic Online AUC Maximization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/08/stochastic-online-auc-maximization/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/08/stochastic-online-auc-maximization/</guid>
      <description>Ying et al., 2016, Stochastic online AUC maximization, NIPS. pdf
IntroductionTopic: online learning algorithms that maximize AUC for large-scale dataChallenge: learning objective is usually defined over a pair of training examples of opposite classesProposed methodAUC optimization can be equivalently formulated as a convex-concave saddle point problem (SPP)From this SPP representation, a stochastic online algorithm (SOLAM) is proposed which has time and space complexity of one datumBackgroundInput space \(\mathcal{X} \subseteq \mathbb{R}^d\)Output space \(\mathcal{Y} = \{ -1, 1\}\)Training data \(\mathbf{z} = \{ (x_i, y_i) \}_{i = 1}^n\): i.</description>
    </item>
    
    <item>
      <title>OAM - Online AUC Maximization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/online-auc-maximization/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/online-auc-maximization/</guid>
      <description>Zhao et al., 2011, Online AUC Maximization, ICML. pdf
MotivationMost studies of online learning measure the performance of a learner by classification accuracy, which is inappropriate for applications where the data are unevenly distributed among different classesAUC maximization needs to optimize the pairwise loss between two instances from different classes, making it unattractive for large-scale applicationsThe authors address this challenge by exploiting the reservoir samplingPreliminaries\((\mathbf{x}_t, y_t)\): the training examples received at the \(t\)-th trial where \(\mathbf{x}_t \in \mathbb{R}^d\) and \(y_t \in \{ -1, 1\}\)Training dataset: \(\mathcal{D} = \{ (\mathbf{x}_i, y_i) \in \mathbb{R} \times \{ -1, 1\} | i \in [T] \}\)The set of positive instances: \(\mathcal{D}_+ = \{ (\mathbf{x}_i^+, 1) | i \in [T_+] \}\)The set of negative instances: \(\mathcal{D}_- = \{ (\mathbf{x}_j^-, -1) | j \in [T_-] \}\)\(T_+\), \(T_-\): the number of positive and negative instancesLinear classifier: \(\mathbf{w} \cdot \mathbf{x}\)\[\begin{equation}\begin{split}\text{AUC} (\mathbf{w}) &amp;amp; = \frac{\sum_{i=1}^{T^+} \sum_{j=1}^{T_-} \mathbb{I} (\mathbf{w} \cdot \mathbf{x}_i^+ &amp;gt; \mathbf{w} \cdot \mathbf{x}_j^-)}{T_+ T_-} \\[10pt]&amp;amp; = 1 - \frac{\sum_{i=1}^{T^+} \sum_{j=1}^{T_-} \mathbb{I} (\mathbf{w} \cdot \mathbf{x}_i^+ \leq \mathbf{w} \cdot \mathbf{x}_j^-)}{T_+ T_-}\end{split}\end{equation}\]</description>
    </item>
    
    <item>
      <title>OPAUC - One-Pass AUC Optimization</title>
      <link>https://haaforever.github.io/blog/post/2020/12/07/one-pass-auc-optimization/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://haaforever.github.io/blog/post/2020/12/07/one-pass-auc-optimization/</guid>
      <description>Gao et al., 2013, One-pass AUC optimization, ICML. pdf
MotivationAUC is measured by a sum of losses defined over pairs of instances from different classesThis causes a problem in applications involving big data or streaming data in which a large volume of data come in a short time periodIn this work, one-pass AUC optimization that requires going through the training data only once without storing the entire training dataset is proposedPreliminariesInstance space \(\mathcal{X} \in \mathbb{R}^d\)Label set \(\mathcal{Y} = \{ -1, +1 \}\)Unknown distribution \(\mathcal{D}\) over \(\mathcal{X} \times \mathcal{Y}\)A training sample of \(n_{+}\) positive instances and \(n_{-}\) negative onesTraining set \(\mathcal{S} = \{ (\mathbf{x}_1^{+}, +1), \cdots, (\mathbf{x}_{n_{+}}^{+}, +1), (\mathbf{x}_1^{-}, -1), \cdots, (\mathbf{x}_{n_{-}}^{-}, -1) \}\)Real-valued function \(f: \mathcal{X} \rightarrow \mathbb{R}\)AUC of \(f\) on \(\mathcal{S}\)\[\begin{equation}\sum_{i=1}^{n_{+}} \sum_{j=1}^{n_{-}} \frac{\mathbb{I}[f(\mathbf{x}_{i}^{+}) &amp;gt; f(\mathbf{x}_{j}^{-})]}{n_{+} n_{-}}\end{equation}\]</description>
    </item>
    
  </channel>
</rss>
