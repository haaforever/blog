---
title: Some Experiments on Loss Functions
date: '2020-12-07'
slug: some-experiments-on-loss-functions
categories:
  - Note
tags:
  - Loss Function
  - Experiment
---


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


```{r, echo = FALSE}
##### Loss functions #####
fn.01 <- function(m) {
  loss <- c()
  loss[m >= 0] <- 0
  loss[m < 0] <- 1
  return(loss)
}
fn.exp <- function(m) {
  loss <- exp(-m)
  return(loss)
}
fn.hinge <- function(m) {
  loss <- sapply(X = m, FUN = function(x) {
    return(max(c(0, 1 - x)))
  })
  return(loss)
}
fn.logit <- function(m) {
  loss <- log(x = 1 + exp(-m), base = 2)
  return(loss)
}
fn.square <- function(m) {
  loss <- (1 - m)^2
  return(loss)
}
fn.savage <- function(m) {
  loss <- 1 / (1 + exp(m))^2
  return(loss)
}
fn.sigmoid <- function(m) {
  loss <- 1 / (1 + exp(m))
  return(loss)
}
fn.bindev <- function(m) {
  loss <- log(x = 1 + exp(-2*m), base = 2)
  return(loss)
}
fn.tan <- function(m) {
  loss <- (2 * atan(m) - 1)^2
  return(loss)
}
##### Gradients of loss functions #####
fn.logit.grad <- function(m) {
  a <- 1 / log(x = 2)
  b <- -1 / (1 + exp(m))
  grad <- a * b
  return(grad)
}
fn.exp.grad <- function(m) {
  grad <- - exp(-m)
  return(grad)
}
fn.hinge.grad <- function(m) {
  grad <- c()
  grad[m <= 1] <- -1
  grad[m > 1] <- 0
  return(grad)
}
fn.square.grad <- function(m) {
  grad <- 2 * (m - 1)
  return(grad)
}
fn.savage.grad <- function(m) {
  a <- -2 * exp(m)
  b <- 1 / (1 + exp(m))^3
  grad <- a * b
  return(grad)
}
fn.sigmoid.grad <- function(m) {
  a <- -1 / (1 + exp(-m))
  b <- 1 / (1 + exp(m))
  grad <- a * b
  return(grad)
}
fn.bindev.grad <- function(m) {
  a <- -2 / log(x = 2)
  b <- 1 / (1 + exp(2 * m))
  grad <- a * b
  return(grad)
}
fn.tan.grad <- function(m) {
  a <- 4 * (2 * atan(x = m) - 1)
  b <- 1 / (1 + m^2)
  grad <- a * b
  return(grad)
}
##### Other functions #####
library(mvtnorm)
fn.gen.dt <- function(n0, n1) {
  x0 <- rmvnorm(n = n0, mean = c(0, 0), sigma = diag(x = 1, nrow = 2))
  x1 <- rmvnorm(n = n1, mean = c(1, 1), sigma = diag(x = 1, nrow = 2))
  x <- rbind(x0, x1) ; rm(x0, x1)
  colnames(x) <- paste0("x", 1:ncol(x))
  y <- c(rep(-1, n0), rep(1, n1))
  return(list(x = x, y = y))
}
fn.h <- function(w0, w, x) {
  h.x <- w0 + as.numeric(rbind(x) %*% cbind(w)) # for vector/matrix
  return(h.x)
}
fn.init <- function(n) {
  r <- runif(n = n, min = -1, max = 1)
  w0 <- r[1]
  w <- r[2:(n)]
  return(list(w0 = w0, w = w))
}
fn.grad <- function(w0, w, x, y, loss) {
  h <- fn.h(w0 = w0, w = w, x = x)
  m <- y * h
  y.grad.l.m <- y * get(x = paste0("fn.", loss, ".grad"))(m)
  grad.w0 <- mean(y.grad.l.m)
  y.grad.l.m.mtx <- matrix(data = y.grad.l.m, ncol = ncol(x), nrow = length(y.grad.l.m), byrow = FALSE)
  grad.w <- colMeans(y.grad.l.m.mtx * x)
  return(list(grad.w0 = grad.w0, grad.w = grad.w))
}
fn.fit <- function(w0, w, x, y, loss, eta, itr) {
  #
  w0.tr <- rep(0, itr + 1)
  w.tr <- matrix(data = 0, nrow = itr + 1, ncol = length(w))
  loss.tr <- rep(0, itr + 1)
  err.tr <- rep(0, itr + 1)
  #
  w0.tr[1] <- w0
  w.tr[1,] <- w
  h <- fn.h(w0 = w0, w = w, x = x)
  m <- y * h
  loss.tr[1] <- mean(get(x = paste0("fn.", loss))(m))
  err.tr[1] <- mean(get(x = paste0("fn.01"))(m))
  #
  for (i in 1:itr) {
    # i <- 1
    w0.old <- w0.tr[i]
    w.old <- w.tr[i,]
    grad <- fn.grad(w0 = w0.old, w = w.old, x = x, y = y, loss = loss)
    w0.new <- w0.old - (eta * grad$"grad.w0")
    w.new <- w.old - (eta * grad$"grad.w")
    #
    w0.tr[i + 1] <- w0.new
    w.tr[i + 1,] <- w.new
    h <- fn.h(w0 = w0.new, w = w.new, x = x)
    m <- y * h
    loss.tr[i + 1] <- mean(get(x = paste0("fn.", loss))(m))
    err.tr[i + 1] <- mean(get(x = paste0("fn.01"))(m))
  }
  #
  return(list(w0 = w0.tr[i + 1],
              w = w.tr[i + 1,],
              trace = list(loss = loss.tr, err = err.tr, w0 = w0.tr, w = w.tr)))
}
```


> Still working


***


## Surrogate loss functions


  * $y \in \{ -1, 1 \}$, $h(x) \in \mathbb{R}$
  * Classification rule
  
$$\begin{equation*}
\hat{y} = \begin{cases}
1 & h(x) \geq 0 \\
-1  & \text{otherwise} 
\end{cases}
\end{equation*}$$

  * Classification margin $m = yh(x) \in \mathbb{R}$
  * Margin-based loss function $L(y, h(x))$


| Loss | Formula |
|:---:|:-----:|
| 0-1 | $I\{yh(x)<0\}$ |
| Logistic | $\log _2 (1+\exp(-yh(x)))$ |
| Exponential | $\exp(-yh(x))$ |
| Hinge | $\max \{ 0, 1-yh(x) \} = \{ 1-yh(x) \}_{+}$ |
| Square | $(1-yh(x))^2$ |
| Savage | $1/(1+\exp(yh(x)))^2$ |
| Sigmoid | $1/(1+\exp(yh(x)))$ |
| Binomial deviance | $\log _2 (1 + \exp(-2yh(x)))$ |
| Tangent | $(2 \tan ^{-1} (yh(x))- 1)^2$ |


```{r, echo = FALSE, fig.align = "center", fig.height = 8, fig.width = 8}
m <- seq(from = -1.5, to = 1.5, by = 0.001)
ylim <- c(0, 5) ; lwd <- 2 ; lty <- 2
plot(x = m, y = fn.01(m = m), type = "l", xlab = expression(yh(x)), ylab = "Loss", ylim = ylim, lwd = lwd)
lines(x = m, y = fn.logit(m = m), lty = lty, lwd = lwd)
lines(x = m, y = fn.exp(m = m), lty = lty, lwd = lwd, col = rainbow(7)[1])
lines(x = m, y = fn.hinge(m = m), lty = lty, lwd = lwd, col = rainbow(7)[2])
lines(x = m, y = fn.square(m = m), lty = lty, lwd = lwd, col = rainbow(7)[3])
lines(x = m, y = fn.savage(m = m), lty = lty, lwd = lwd, col = rainbow(7)[4])
lines(x = m, y = fn.sigmoid(m = m), lty = lty, lwd = lwd, col = rainbow(7)[5])
lines(x = m, y = fn.bindev(m = m), lty = lty, lwd = lwd, col = rainbow(7)[6])
lines(x = m, y = fn.tan(m = m), lty = lty, lwd = lwd, col = rainbow(7)[7])
legend("topright", legend = c("0-1", "Logit", "Exp", "Hinge", "Sqaure", "Savage", "Sigmoid", "BinDev", "Tangent"), bty = "n", lty = c(1, rep(lty, 8)), lwd = lwd, col = c(1, 1, rainbow(7)))
```


***


## Data description
  * Two 2D Gaussian distribution
  * Positive samples $X^{+} \sim N(\mu^{+}, I_2)$ where $\mu^{+} = (1,1)^T$
  * Negative samples $X^{-} \sim N(\mu^{-}, I_2)$ where $\mu^{+} = (0,0)^T$
  * $I_2$: $(2 \times 2)$ identity matrix
  * Same prior distribution
  
```{r, echo = FALSE, fig.align = "center", fig.height = 8, fig.width = 8}
n.pos <- 1000
n.neg <- 1000
x.pos <- mvtnorm::rmvnorm(n = n.pos, mean = c(1, 1), sigma = diag(x = 1, nrow = 2))
x.neg <- mvtnorm::rmvnorm(n = n.neg, mean = c(0, 0), sigma = diag(x = 1, nrow = 2))
x <- rbind(x.pos, x.neg)
y <- c(rep(1, n.pos), rep(-1, n.neg))
col <- c()
col[y == 1] <- "blue"
col[y == -1] <- "red"
plot(x, xlab = "x1", ylab = "x2", col = col)
```


***


## Experiments on error minimization
  * Find the optimal linear classifier $h^{\star}$
  * Linear classifier $h(\mathbf{x}) = w_{0} + w_1x_1 + w_2x_2 = w_{0} + \mathbf{w} \cdot \mathbf{x}$
  * Given training dataset $D = \{ (\mathbf{x}_i,y_i) \}_{i=1}^{n} = \{ (\mathbf{x}^{+}_{1},1), \cdots, (\mathbf{x}^{+}_{n^{+}},1), (\mathbf{x}^{-}_{1},-1), \cdots, (\mathbf{x}^{-}_{n^{-}},-1) \}$
  * Original formulation
    
  $$\argmin_{w_{0}, \mathbf{w}} \Pr (Yh(X)<0)$$
    
  * Empirical loss minimization
  
  $$\argmin_{w_{0}, \mathbf{w}} \sum_{i=1}^{n} L(y_i,h(\mathbf{x}_i))$$

  
### Learning curve for each loss function
  * Training size: $n^{+} = 1,000$, $n^{-} = 1,000$
  * Gradient descent method with full-batch update
  * Worst starting point: $(w_{0}^{(0)}, w_{1}^{(0)}, w_{2}^{(0)}) = (0, -1, 1)$
  * Learning rate $\eta \in \{ 0.05, 0.1, 0.5 \}$
  * Epoch: 200
  * Estimation of test error: $n^{+} = 100,000$, $n^{-} = 100,000$
  * Right click the image and open with new tab
    
    
```{r, echo = FALSE, fig.align = "center", fig.height = 8, fig.width = 16}
dt <- fn.gen.dt(n0 = 1000, n1 = 1000)
x <- dt$"x"
y <- dt$"y"
dt.ts <- fn.gen.dt(n0 = 100000, n1 = 100000)
x.ts <- dt.ts$"x"
y.ts <- dt.ts$"y"
w0 <- 0
w <- c(-1, 1)
lwd <- 2
loss <- c("logit", "exp", "hinge", "square", "savage", "sigmoid", "bindev", "tan")




##### Eta = 0.05 #####
res <- list()
for (i in 1:length(loss)) {
  # print(loss[i])
  res[[i]] <- fn.fit(w0 = w0, w = w, x = x, y = y, loss = loss[i], eta = 0.05, itr = 200)
}
names(res) <- loss
# Loss curve
layout(matrix(data = 1:2, nrow = 1, ncol = 2))
ylim <- c(0.2, 3)
plot(res[[1]]$"trace"$"loss", type = "l", xlab = "Epoch", ylab = "Loss", ylim = ylim, lwd = lwd, lty = 1)
title(main = expression(paste("Loss curve, ", eta, "= 0.05")))
for (i in 2:length(loss)) {
  lines(res[[i]]$"trace"$"loss", type = "l", col = rainbow(7)[i], lwd = lwd, lty = 1)
}
legend("topright",
       legend = c("Logit", "Exp", "Hinge", "Square", "Savage", "Sigmoid", "BinDev", "Tan"),
       bty = "n", lty = 1, lwd = lwd, col = c(1, rainbow(7)))
# Training curve
ylim <- c(0.23, 0.52)
plot(res[[1]]$"trace"$"err", type = "l", xlab = "Epoch", ylab = "Error", ylim = ylim, lwd = lwd, lty = 1)
title(main = "Error curve (real = train, dashed = test)")
for (i in 2:length(loss)) {
  lines(res[[i]]$"trace"$"err", type = "l", col = rainbow(7)[i], lwd = lwd, lty = 1)
}
legend("topright",
       legend = c("Logit", "Exp", "Hinge", "Square", "Savage", "Sigmoid", "BinDev", "Tan"),
       bty = "n", lty = 1, lwd = lwd, col = c(1, rainbow(7)))
# Test curve
err.ts <- list()
for (i in 1:length(res)) {
  err <- c()
  for (j in 1:201) {
    w0.ts <- res[[i]]$"trace"$"w0"[j]
    w.ts <- res[[i]]$"trace"$"w"[j,]
    yhat.ts <- fn.h(w0 = w0.ts, w = w.ts, x = x.ts)
    yhat.ts[yhat.ts >= 0] <- 1
    yhat.ts[yhat.ts < 0] <- -1
    err[j] <- sum(y.ts != yhat.ts) / 200000
  }
  err.ts[[i]] <- err
}
lines(err.ts[[1]], type = "l", lwd = lwd, lty = 2)
for (i in 2:length(err.ts)) {
  lines(err.ts[[i]], type = "l", col = rainbow(7)[i], lwd = lwd, lty = 2)
}




##### Eta = 0.1 #####
res <- list()
for (i in 1:length(loss)) {
  # print(loss[i])
  res[[i]] <- fn.fit(w0 = w0, w = w, x = x, y = y, loss = loss[i], eta = 0.1, itr = 200)
}
names(res) <- loss
# Loss curve
layout(matrix(data = 1:2, nrow = 1, ncol = 2))
ylim <- c(0.2, 3)
plot(res[[1]]$"trace"$"loss", type = "l", xlab = "Epoch", ylab = "Loss", ylim = ylim, lwd = lwd, lty = 1)
title(main = expression(paste("Loss curve, ", eta, "= 0.1")))
for (i in 2:length(loss)) {
  lines(res[[i]]$"trace"$"loss", type = "l", col = rainbow(7)[i], lwd = lwd, lty = 1)
}
legend("topright",
       legend = c("Logit", "Exp", "Hinge", "Square", "Savage", "Sigmoid", "BinDev", "Tan"),
       bty = "n", lty = 1, lwd = lwd, col = c(1, rainbow(7)))
# Training curve
ylim <- c(0.23, 0.52)
plot(res[[1]]$"trace"$"err", type = "l", xlab = "Epoch", ylab = "Error", ylim = ylim, lwd = lwd, lty = 1)
title(main = "Error curve (real = train, dashed = test)")
for (i in 2:length(loss)) {
  lines(res[[i]]$"trace"$"err", type = "l", col = rainbow(7)[i], lwd = lwd, lty = 1)
}
legend("topright",
       legend = c("Logit", "Exp", "Hinge", "Square", "Savage", "Sigmoid", "BinDev", "Tan"),
       bty = "n", lty = 1, lwd = lwd, col = c(1, rainbow(7)))
# Test curve
err.ts <- list()
for (i in 1:length(res)) {
  err <- c()
  for (j in 1:201) {
    w0.ts <- res[[i]]$"trace"$"w0"[j]
    w.ts <- res[[i]]$"trace"$"w"[j,]
    yhat.ts <- fn.h(w0 = w0.ts, w = w.ts, x = x.ts)
    yhat.ts[yhat.ts >= 0] <- 1
    yhat.ts[yhat.ts < 0] <- -1
    err[j] <- sum(y.ts != yhat.ts) / 200000
  }
  err.ts[[i]] <- err
}
lines(err.ts[[1]], type = "l", lwd = lwd, lty = 2)
for (i in 2:length(err.ts)) {
  lines(err.ts[[i]], type = "l", col = rainbow(7)[i], lwd = lwd, lty = 2)
}




##### Eta = 0.5 #####
res <- list()
for (i in 1:length(loss)) {
  # print(loss[i])
  res[[i]] <- fn.fit(w0 = w0, w = w, x = x, y = y, loss = loss[i], eta = 0.5, itr = 200)
}
names(res) <- loss
# Loss curve
layout(matrix(data = 1:2, nrow = 1, ncol = 2))
ylim <- c(0.2, 3)
plot(res[[1]]$"trace"$"loss", type = "l", xlab = "Epoch", ylab = "Loss", ylim = ylim, lwd = lwd, lty = 1)
title(main = expression(paste("Loss curve, ", eta, "= 0.5")))
for (i in 2:length(loss)) {
  lines(res[[i]]$"trace"$"loss", type = "l", col = rainbow(7)[i], lwd = lwd, lty = 1)
}
legend("topright",
       legend = c("Logit", "Exp", "Hinge", "Square", "Savage", "Sigmoid", "BinDev", "Tan"),
       bty = "n", lty = 1, lwd = lwd, col = c(1, rainbow(7)))
# Training curve
ylim <- c(0.23, 0.52)
plot(res[[1]]$"trace"$"err", type = "l", xlab = "Epoch", ylab = "Error", ylim = ylim, lwd = lwd, lty = 1)
title(main = "Error curve (real = train, dashed = test)")
for (i in 2:length(loss)) {
  lines(res[[i]]$"trace"$"err", type = "l", col = rainbow(7)[i], lwd = lwd, lty = 1)
}
legend("topright",
       legend = c("Logit", "Exp", "Hinge", "Square", "Savage", "Sigmoid", "BinDev", "Tan"),
       bty = "n", lty = 1, lwd = lwd, col = c(1, rainbow(7)))
# Test curve
err.ts <- list()
for (i in 1:length(res)) {
  err <- c()
  for (j in 1:201) {
    w0.ts <- res[[i]]$"trace"$"w0"[j]
    w.ts <- res[[i]]$"trace"$"w"[j,]
    yhat.ts <- fn.h(w0 = w0.ts, w = w.ts, x = x.ts)
    yhat.ts[yhat.ts >= 0] <- 1
    yhat.ts[yhat.ts < 0] <- -1
    err[j] <- sum(y.ts != yhat.ts) / 200000
  }
  err.ts[[i]] <- err
}
lines(err.ts[[1]], type = "l", lwd = lwd, lty = 2)
for (i in 2:length(err.ts)) {
  lines(err.ts[[i]], type = "l", col = rainbow(7)[i], lwd = lwd, lty = 2)
}
```


### Noisy environment
### Stochastic update


