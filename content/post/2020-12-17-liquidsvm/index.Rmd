---
title: liquidSVM
author: ''
date: '2020-12-17'
slug: liquidsvm
categories:
  - R
tags:
  - R
  - SVM
output:
  blogdown::html_page:
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.width = 6, fig.height = 6, warning = FALSE)
```


Steinwart and Thomann, 2017, **liquidSVM: A Fast and Versatile SVM package**, *arXiv*. [pdf](https://arxiv.org/pdf/1702.06899.pdf) \
Vignette 1: Documentation [pdf](https://cran.r-project.org/web/packages/liquidSVM/vignettes/documentation.html) \
Vignette 2: Demo [pdf](https://cran.r-project.org/web/packages/liquidSVM/vignettes/demo.html)


***


## Key Features
  * Written **C++**
  * Fully integrated hyper-parameter selection
  * Extreme speed on both small and large data sets
  * Multi-class classification, ROC, and Neyman-Pearson learning, least-squares, quantile, and expectile regression
  * Flexible user interface ranging from a **C++** API and a command line version to bindings
for **R**, **MATLAB**, **Java**, **Python**, and **Spark**


## Installation
```{r}
# install.packages("liquidSVM",configure.args="native")
# install.packages('liquidSVM', configure.args="native /usr/local/cuda") # CUDA path
library(liquidSVM)
```


***


## Application Cycle
  1. Training phase: various SVM models are created and validated
  2. Selection phase: the SVM models that best satisfy a certain criterion are selected
  3. Test phase: in which the selected models are applied to test data
  * These phases are based upon several components, which can be freely combined using different components: solvers, hyper-parameter selection, working sets
  * All of these can be configured (see Configuration)


***


## Type of SVM
  * *mcSVM*: binary and multi-class classification
  * *lsSVM*: least squares regression
  * *nplSVM*: Neyman-Pearson learning to classify with a specified rate on one type of error
  * *rocSVM*: Receivert Operating Characteristic (ROC) curve to solve multiple weighted binary classification problems.
  * *qtSVM*: quantile regression
  * *exSVM*: expectile regression
  * *bsSVM*: bootstrapping


***


## Configuration Parameters
  * *display* = 1
    + The amount of output of you see at the screen
    + The larger its value is, the more you see
  * *scale* = TRUE
    + [0, 1]
    + The training then is performed using these scaled values and any testing data is scaled transparently as well
  * *threads*
    + The number of cores used for computing the kernel matrices, the validation error, and the test error
    + *threads* = 0 (default) means that all physical cores of your CPU run one thread
    + *threads* = -1 means that all but one physical cores of your CPU run one thread
  * *partition_choice*
    + The way the input space is partitioned
    + This allows larger data sets for which the kernel matrix does not fit into memory
    + *partition_choice* = 0 (default) disables partitioning
    + *partition_choice* = 6 gives usually highest speed
    + *partition_choice* = 5 gives usually the best test error
  * *grid_choice*
    + The size of the hyper-parameter grid used during the training phase
    + Larger values correspond to larger grids
    + By default, a 10x10 grid is used
  * *adaptivity_control*
    + Whether an adaptive grid search heuristic is employed
    + Larger values lead to more aggressive strategies
    + *adaptivity_control* = 0 (default) disables the heuristic
  * *random_seed*
    + This parameter determines the seed for the random generator
    + *random_seed* = -1 uses the internal timer create the seed
    + All other values lead to repeatable behavior of the svm
  * *folds*
    + How many folds should be used


***


##  Hyperparameter Grid
  * *liquidSVM* has a built-in a cross-validation scheme to calculate validation errors for many values of these hyperparameters and then to choose the best pair
  * *gamma*: the bandwith of the kernel
  * *lambda*: regularization parameter in front of the norm term
  * *gamma_steps*, *min_gamma*, *max_gamma*: the interval between *min_gamma* and *max_gamma*
  * *gammas* = c(0.1,1,10,100) will do these four gamma values
  * *lambda_steps*, *min_lambda*, *max_lambda*: the interval between *min_lambda* and *max_lambda*
  * *lambdas* = c(0.1,1,10,100) will do these four lambda values
  * *c_values*
    + the classical term in front of the empirical error term
    + *c_values*=c(0.1,1,10,100) will do these four cost values (basically inverse of lambdas)
  * Using *grid_choice* allows for some general choices of these parameters
  * *grid_choice* = 0
    + *gamma_steps*, *min_gamma*, *max_gamma* = 10, 0.2, 5
    + *lambda_steps*, *min_lambda*, *max_lambda* = 10, 0.001, 0.01
  * *grid_choice* = 1
    + *gamma_steps*, *min_gamma*, *max_gamma* = 15, 0.1, 10
    + *lambda_steps*, *min_lambda*, *max_lambda* = 15, 0.0001, 0.01
  * *grid_choice* = 2
    + *gamma_steps*, *min_gamma*, *max_gamma* = 20, 0.05, 20
    + *lambda_steps*, *min_lambda*, *max_lambda* = 20, 0.00001, 0.01
  * *grid_choice* = -1
    + *gammas* = c(10.0, 5.0, 2.0, 1.0, 0.5, 0.25, 0.1, 0.05)
    + *lambdas* = c(1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001)
   * *grid_choice* -2
    + *gammas* = c(10.0, 5.0, 2.0, 1.0, 0.5, 0.25, 0.1, 0.05)
    + *c_values* = c(0.01, 0.1, 1, 10, 100, 1000, 10000)


***


## Kernel matrix
```{r}
x <- mvtnorm::rmvnorm(n = 10000, mean = c(0, 0), sigma = diag(x = 1, nrow = 2))
n.thrd <- c(-1, 0, 1, 2, 4, 8)
# Check running time
for (i in 1:length(n.thrd)) {
  print(system.time(k <- kern(data = x, gamma = 1, type = "gaussian.rbf", threads = n.thrd[i])))
  rm(k) 
}
```


***


## Input format
```{r}
# liquidData()
dt.liquid.iris <- ttsplit(data = iris, target = "Species", testProb = 0.3, stratified = TRUE)
is(dt.liquid.iris)
dt.liquid.iris
names(dt.liquid.iris)
head(dt.liquid.iris$"test")
```


***


## Simple Classification Example
```{r}
##### Data Generation #####
fn.gen.bincls.2gauss <- function(n.neg, n.pos, mu, d) {
  x.0 <- mvtnorm::rmvnorm(n = n.neg, mean = rep(0, d), sigma = diag(1, d))
  x.1 <- mvtnorm::rmvnorm(n = n.pos, mean = rep(mu, d), sigma = diag(1, d))
  y.0 <- rep(-1, n.neg)
  y.1 <- rep(1, n.pos)
  dt <- data.frame(rbind(x.0, x.1), y = c(y.0, y.1))
  colnames(dt)[1:d] <- paste0("x", 1:d)
  return(dt)
}
dt <- fn.gen.bincls.2gauss(n.neg = 1000, n.pos = 1000, mu = 1, d = 2)
plot(x = dt$"x1", y = dt$"x2", col = dt$"y" + 3, xlab = "x1", ylab = "x2")
```

```{r}
##### Input Format #####
dt.liquid <- ttsplit(data = dt, target = "y", testProb = 0.3, stratified = TRUE)
# default: (train, test = 0.8, 0.2)
is(dt.liquid)
object.size(dt) ; object.size(dt.liquid) # wow
# View(dt.liquid)
```

```{r}
##### Train, Select, Prediction #####
model.svm <- svm(y ~., dt.liquid$"train",
                 scenario = "mcSVM", d = 1, scale = TRUE,
                 threads = 0, gird_choice = -1)
selectSVMs(model.svm)
yhat <- predict(object = model.svm, newdata = dt.liquid$"test")
table(dt.liquid$test$"y", yhat)
```

