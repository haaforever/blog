---
title: Stochastic Online AUC Maximization
date: '2020-12-08'
slug: stochastic-online-auc-maximization
categories:
  - Paper
tags:
  - NIPS
  - AUC
  - Optimization
  - Online Learning
---

<link href="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/anchor-sections/anchor-sections.js"></script>
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/plotly-binding/plotly.js"></script>
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/mathjax/cdn.js"></script>
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/typedarray/typedarray.min.js"></script>
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/jquery/jquery.min.js"></script>
<link href="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/crosstalk/js/crosstalk.min.js"></script>
<link href="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="{{< relref "post/2020-12-08-stochastic-online-auc-maximization/index.html" >}}index_files/plotly-main/plotly-latest.min.js"></script>



<p>Ying et al., 2016, <strong>Stochastic online AUC maximization</strong>, <em>NIPS</em>. <a href="https://papers.nips.cc/paper/2016/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf">pdf</a></p>
<hr />
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<ul>
<li>Topic: online learning algorithms that maximized AUC for large-scale data</li>
<li>Challenge: learning objective is usually defined over a pair of training examples of opposite classes</li>
<li>Proposed method
<ul>
<li>AUC optimization can be equivalently formulated as a convex-concave saddle point problem (SPP)</li>
<li>From this SPP representation, a stochastic online algorithm (SOLAM) is proposed which has time and space complexity of one datum</li>
</ul></li>
</ul>
<hr />
</div>
<div id="background" class="section level2">
<h2>Background</h2>
<ul>
<li>Input space <span class="math inline">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span></li>
<li>Output space <span class="math inline">\(\mathcal{Y} = \{ -1, 1\}\)</span></li>
<li>Training data <span class="math inline">\(\mathbf{z} = \{ (x_i, y_i) \}_{i = 1}^n\)</span>: i.i.d. sample from unknown distribution <span class="math inline">\(\rho\)</span> on <span class="math inline">\(\mathcal{Z} = \mathcal{X} \times \mathcal{Y}\)</span></li>
<li>Scoring function <span class="math inline">\(f: \mathcal{X} \to \mathbb{R}\)</span></li>
<li>AUC: the probability of a positive sample ranks higher than a negative sample</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathop{\mathrm{argmax}}_f \; \text{AUC}(f) &amp; = \mathop{\mathrm{argmax}}_f \; \Pr(f(x) \geq f(x&#39;) | y = 1, y&#39; = -1) \\[10pt]
&amp; = \mathop{\mathrm{argmin}}_f \; \mathbb{E} [\mathbb{I} (f(x&#39;) - f(x) &gt; 0) | y = 1, y&#39; = -1]
\end{split}
\end{equation}\]</span></p>
<ul>
<li>Squared loss has been shown to be statistically consistent with AUC (Gao and Zhou, 2015)</li>
<li>In this work, squared loss and linear classifier are used</li>
<li>Let <span class="math inline">\(p = \Pr (y=1)\)</span></li>
<li>For any random variable <span class="math inline">\(\xi (z)\)</span>, conditional expectation is defined by <span class="math inline">\(\mathbb{E} [\xi(z) | y=1] = \frac{1}{p} \int \int \xi(z) \mathbb{I}(y = 1) d\rho(z)\)</span></li>
<li>Bounded solution <span class="math inline">\(\lVert \mathbf{w} \rVert \leq R\)</span></li>
<li>AUC maximization can be formulated by</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp; \mathop{\mathrm{argmin}}_{\lVert \mathbf{w} \rVert \leq R} \; \mathbb{E} [(1 - \mathbf{w}^\text{T} (x - x&#39;))^2 | y = 1, y&#39; = -1] \\[10pt]
= \; &amp; \mathop{\mathrm{argmin}}_{\lvert \mathbf{w} \rVert \leq R} \; \frac{1}{p(1-p)} \int \int_{\mathcal{Z} \times \mathcal{Z}} (1 - \mathbf{w}^\text{T} (x - x&#39;))^2 \mathbb{I}(y = 1) \mathbb{I}(y&#39; = -1) d\rho(z) d\rho(z&#39;)
\end{split}
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(n^+\)</span>, <span class="math inline">\(n^-\)</span>: the number of instances in the positive and negative classes</li>
<li>Applying empirical risk minimization principle</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mathop{\mathrm{argmin}}_{\lVert \mathbf{w} \rVert \leq R} \frac{1}{n^+ n^-} \sum_{i=1}^{n} \sum_{j=1}^{n} (1 - \mathbf{w}^\text{T} (x_i - x_j))^2 \mathbb{I}(y_i = 1 \land y&#39;_j = -1)
\end{equation}\]</span></p>
<hr />
</div>
<div id="saddle-point-problems-spp" class="section level2">
<h2>Saddle Point Problems (SPP)</h2>
<p><span class="math display">\[\begin{equation}
\min_{u \in \mathcal{U}} \; \max_{v \in \mathcal{V}} \; g(u, v)
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(g\)</span> is a smooth convex-concave function</li>
<li><span class="math inline">\(g(\cdot, v)\)</span> is convex for all <span class="math inline">\(v \in \mathcal{V}\)</span></li>
<li><span class="math inline">\(g(u, \cdot)\)</span> is concave for all <span class="math inline">\(u \in \mathcal{U}\)</span></li>
<li>A saddle point solution to the problem is a pair <span class="math inline">\((u^{\star}, v^{\star}) \in \mathcal{U} \times \mathcal{V}\)</span> such that</li>
</ul>
<p><span class="math display">\[\begin{equation}
g(u^{\star}, v) \leq g(u^{\star}, v^{\star}) \leq g(u, v^{\star}) \quad \forall \; u \in \mathcal{U}, \; v \in \mathcal{V}
\end{equation}\]</span></p>
<div id="htmlwidget-1" style="width:768px;height:768px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"4d80f354705":["function () ","plotlyVisDat"]},"cur_data":"4d80f354705","attrs":{"4d80f354705":{"x":[-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],"y":[-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],"z":[[0,0.19,0.36,0.51,0.64,0.75,0.84,0.91,0.96,0.99,1,0.99,0.96,0.91,0.84,0.75,0.64,0.51,0.36,0.19,0],[-0.19,0,0.17,0.32,0.45,0.56,0.65,0.72,0.77,0.8,0.81,0.8,0.77,0.72,0.65,0.56,0.45,0.32,0.17,-2.22044604925031e-16,-0.19],[-0.36,-0.17,0,0.15,0.28,0.39,0.48,0.55,0.6,0.63,0.64,0.63,0.6,0.55,0.48,0.39,0.28,0.15,0,-0.17,-0.36],[-0.51,-0.32,-0.15,0,0.13,0.24,0.33,0.4,0.45,0.48,0.49,0.48,0.45,0.4,0.33,0.24,0.13,-3.33066907387547e-16,-0.15,-0.32,-0.51],[-0.64,-0.45,-0.28,-0.13,0,0.11,0.2,0.27,0.32,0.35,0.36,0.35,0.32,0.27,0.2,0.11,-1.11022302462516e-16,-0.13,-0.28,-0.45,-0.64],[-0.75,-0.56,-0.39,-0.24,-0.11,0,0.0900000000000001,0.16,0.21,0.24,0.25,0.24,0.21,0.16,0.0899999999999999,0,-0.11,-0.24,-0.39,-0.56,-0.75],[-0.84,-0.65,-0.48,-0.33,-0.2,-0.0900000000000001,0,0.07,0.12,0.15,0.16,0.15,0.12,0.0699999999999999,-1.94289029309402e-16,-0.0900000000000001,-0.2,-0.33,-0.48,-0.65,-0.84],[-0.91,-0.72,-0.55,-0.4,-0.27,-0.16,-0.07,0,0.05,0.08,0.09,0.0799999999999999,0.0499999999999999,-6.93889390390723e-17,-0.0700000000000002,-0.16,-0.27,-0.4,-0.55,-0.72,-0.91],[-0.96,-0.77,-0.6,-0.45,-0.32,-0.21,-0.12,-0.05,0,0.03,0.04,0.03,-9.0205620750794e-17,-0.05,-0.12,-0.21,-0.32,-0.45,-0.6,-0.77,-0.96],[-0.99,-0.8,-0.63,-0.48,-0.35,-0.24,-0.15,-0.08,-0.03,0,0.01,-2.25514051876985e-17,-0.0300000000000001,-0.08,-0.15,-0.24,-0.35,-0.48,-0.63,-0.8,-0.99],[-1,-0.81,-0.64,-0.49,-0.36,-0.25,-0.16,-0.09,-0.04,-0.01,0,-0.01,-0.0400000000000001,-0.09,-0.16,-0.25,-0.36,-0.49,-0.64,-0.81,-1],[-0.99,-0.8,-0.63,-0.48,-0.35,-0.24,-0.15,-0.0799999999999999,-0.03,2.25514051876985e-17,0.01,0,-0.0300000000000001,-0.08,-0.15,-0.24,-0.35,-0.48,-0.63,-0.8,-0.99],[-0.96,-0.77,-0.6,-0.45,-0.32,-0.21,-0.12,-0.0499999999999999,9.0205620750794e-17,0.0300000000000001,0.0400000000000001,0.0300000000000001,0,-0.05,-0.12,-0.21,-0.32,-0.45,-0.6,-0.77,-0.96],[-0.91,-0.72,-0.55,-0.4,-0.27,-0.16,-0.0699999999999999,6.93889390390723e-17,0.05,0.08,0.09,0.08,0.05,0,-0.0700000000000001,-0.16,-0.27,-0.4,-0.55,-0.72,-0.91],[-0.84,-0.65,-0.48,-0.33,-0.2,-0.0899999999999999,1.94289029309402e-16,0.0700000000000002,0.12,0.15,0.16,0.15,0.12,0.0700000000000001,0,-0.0899999999999999,-0.2,-0.33,-0.48,-0.65,-0.84],[-0.75,-0.56,-0.39,-0.24,-0.11,0,0.0900000000000001,0.16,0.21,0.24,0.25,0.24,0.21,0.16,0.0899999999999999,0,-0.11,-0.24,-0.39,-0.56,-0.75],[-0.64,-0.45,-0.28,-0.13,1.11022302462516e-16,0.11,0.2,0.27,0.32,0.35,0.36,0.35,0.32,0.27,0.2,0.11,0,-0.13,-0.28,-0.45,-0.64],[-0.51,-0.32,-0.15,3.33066907387547e-16,0.13,0.24,0.33,0.4,0.45,0.48,0.49,0.48,0.45,0.4,0.33,0.24,0.13,0,-0.15,-0.32,-0.51],[-0.36,-0.17,0,0.15,0.28,0.39,0.48,0.55,0.6,0.63,0.64,0.63,0.6,0.55,0.48,0.39,0.28,0.15,0,-0.17,-0.36],[-0.19,2.22044604925031e-16,0.17,0.32,0.45,0.56,0.65,0.72,0.77,0.8,0.81,0.8,0.77,0.72,0.65,0.56,0.45,0.32,0.17,0,-0.19],[0,0.19,0.36,0.51,0.64,0.75,0.84,0.91,0.96,0.99,1,0.99,0.96,0.91,0.84,0.75,0.64,0.51,0.36,0.19,0]],"colorscale":"Picnic","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"$f(u, v) = u^2 - v^2$","scene":{"xaxis":{"title":"v"},"yaxis":{"title":"u"},"zaxis":{"title":"f"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"colorbar":{"title":"","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":"Picnic","showscale":true,"x":[-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],"y":[-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],"z":[[0,0.19,0.36,0.51,0.64,0.75,0.84,0.91,0.96,0.99,1,0.99,0.96,0.91,0.84,0.75,0.64,0.51,0.36,0.19,0],[-0.19,0,0.17,0.32,0.45,0.56,0.65,0.72,0.77,0.8,0.81,0.8,0.77,0.72,0.65,0.56,0.45,0.32,0.17,-2.22044604925031e-16,-0.19],[-0.36,-0.17,0,0.15,0.28,0.39,0.48,0.55,0.6,0.63,0.64,0.63,0.6,0.55,0.48,0.39,0.28,0.15,0,-0.17,-0.36],[-0.51,-0.32,-0.15,0,0.13,0.24,0.33,0.4,0.45,0.48,0.49,0.48,0.45,0.4,0.33,0.24,0.13,-3.33066907387547e-16,-0.15,-0.32,-0.51],[-0.64,-0.45,-0.28,-0.13,0,0.11,0.2,0.27,0.32,0.35,0.36,0.35,0.32,0.27,0.2,0.11,-1.11022302462516e-16,-0.13,-0.28,-0.45,-0.64],[-0.75,-0.56,-0.39,-0.24,-0.11,0,0.0900000000000001,0.16,0.21,0.24,0.25,0.24,0.21,0.16,0.0899999999999999,0,-0.11,-0.24,-0.39,-0.56,-0.75],[-0.84,-0.65,-0.48,-0.33,-0.2,-0.0900000000000001,0,0.07,0.12,0.15,0.16,0.15,0.12,0.0699999999999999,-1.94289029309402e-16,-0.0900000000000001,-0.2,-0.33,-0.48,-0.65,-0.84],[-0.91,-0.72,-0.55,-0.4,-0.27,-0.16,-0.07,0,0.05,0.08,0.09,0.0799999999999999,0.0499999999999999,-6.93889390390723e-17,-0.0700000000000002,-0.16,-0.27,-0.4,-0.55,-0.72,-0.91],[-0.96,-0.77,-0.6,-0.45,-0.32,-0.21,-0.12,-0.05,0,0.03,0.04,0.03,-9.0205620750794e-17,-0.05,-0.12,-0.21,-0.32,-0.45,-0.6,-0.77,-0.96],[-0.99,-0.8,-0.63,-0.48,-0.35,-0.24,-0.15,-0.08,-0.03,0,0.01,-2.25514051876985e-17,-0.0300000000000001,-0.08,-0.15,-0.24,-0.35,-0.48,-0.63,-0.8,-0.99],[-1,-0.81,-0.64,-0.49,-0.36,-0.25,-0.16,-0.09,-0.04,-0.01,0,-0.01,-0.0400000000000001,-0.09,-0.16,-0.25,-0.36,-0.49,-0.64,-0.81,-1],[-0.99,-0.8,-0.63,-0.48,-0.35,-0.24,-0.15,-0.0799999999999999,-0.03,2.25514051876985e-17,0.01,0,-0.0300000000000001,-0.08,-0.15,-0.24,-0.35,-0.48,-0.63,-0.8,-0.99],[-0.96,-0.77,-0.6,-0.45,-0.32,-0.21,-0.12,-0.0499999999999999,9.0205620750794e-17,0.0300000000000001,0.0400000000000001,0.0300000000000001,0,-0.05,-0.12,-0.21,-0.32,-0.45,-0.6,-0.77,-0.96],[-0.91,-0.72,-0.55,-0.4,-0.27,-0.16,-0.0699999999999999,6.93889390390723e-17,0.05,0.08,0.09,0.08,0.05,0,-0.0700000000000001,-0.16,-0.27,-0.4,-0.55,-0.72,-0.91],[-0.84,-0.65,-0.48,-0.33,-0.2,-0.0899999999999999,1.94289029309402e-16,0.0700000000000002,0.12,0.15,0.16,0.15,0.12,0.0700000000000001,0,-0.0899999999999999,-0.2,-0.33,-0.48,-0.65,-0.84],[-0.75,-0.56,-0.39,-0.24,-0.11,0,0.0900000000000001,0.16,0.21,0.24,0.25,0.24,0.21,0.16,0.0899999999999999,0,-0.11,-0.24,-0.39,-0.56,-0.75],[-0.64,-0.45,-0.28,-0.13,1.11022302462516e-16,0.11,0.2,0.27,0.32,0.35,0.36,0.35,0.32,0.27,0.2,0.11,0,-0.13,-0.28,-0.45,-0.64],[-0.51,-0.32,-0.15,3.33066907387547e-16,0.13,0.24,0.33,0.4,0.45,0.48,0.49,0.48,0.45,0.4,0.33,0.24,0.13,0,-0.15,-0.32,-0.51],[-0.36,-0.17,0,0.15,0.28,0.39,0.48,0.55,0.6,0.63,0.64,0.63,0.6,0.55,0.48,0.39,0.28,0.15,0,-0.17,-0.36],[-0.19,2.22044604925031e-16,0.17,0.32,0.45,0.56,0.65,0.72,0.77,0.8,0.81,0.8,0.77,0.72,0.65,0.56,0.45,0.32,0.17,0,-0.19],[0,0.19,0.36,0.51,0.64,0.75,0.84,0.91,0.96,0.99,1,0.99,0.96,0.91,0.84,0.75,0.64,0.51,0.36,0.19,0]],"type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<hr />
</div>
<div id="equivalent-representation-as-a-spp" class="section level2">
<h2>Equivalent Representation as a SPP</h2>
<ul>
<li><strong>Theorem 1.</strong> The AUC optimization is equivalent to</li>
</ul>
<p><span class="math display">\[\begin{equation}
\min_{\begin{array}{c} \lVert \mathbf{w} \rVert \leq R \\[-2pt] (a, b) \in \mathbb{R}^2 \end{array}} \; \max_{\alpha \in \mathbb{R}} \; f(\mathbf{w}, a, b, \alpha) := \int_{\mathcal{Z}} F(\mathbf{w}, a, b, \alpha ; z) d\rho(z)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{split}
F(\mathbf{w}, a, b, \alpha ; z) &amp; = (1 - p) (\mathbf{w}^{\text{T}} x - a)^2 \mathbb{I}(y = 1) + p (\mathbf{w}^{\text{T}} x - b)^2 \mathbb{I}(y = -1) \\[10pt]
&amp; + 2 (1 + \alpha) (p \mathbf{w}^{\text{T}} x \mathbb{I}(y = -1) - (1 - p) \mathbf{w}^{\text{T}} x \mathbb{I}(y = 1)) - p (1 - p) \alpha^2
\end{split}
\end{equation}\]</span></p>
<ul>
<li>Proof omitted (see the paper for more details)</li>
</ul>
<hr />
</div>
<div id="stocastic-first-order-online-algorithm" class="section level2">
<h2>Stocastic First-Order Online Algorithm</h2>
<ul>
<li>Gradient descent in the primal variable <span class="math inline">\((\mathbf{w}, a, b)\)</span></li>
<li>Gradient ascent in the dual variable <span class="math inline">\(\alpha\)</span></li>
<li>Let <span class="math inline">\(v = (\mathbf{w}^{\text{T}}, a, b)^{\text{T}} \in \mathbb{R}^{d+2}\)</span></li>
<li><span class="math inline">\(\mathbf{w} \in \mathbb{R}^d\)</span>, <span class="math inline">\(a, b, \alpha \in \mathbb{R}\)</span>, <span class="math inline">\(z = (x, y) \in \mathbb{Z}\)</span></li>
<li>We need the knowledge of the unknown probability <span class="math inline">\(p = \Pr(y = 1)\)</span>, or <span class="math inline">\(\hat{p}_t = \sum_{i=1}^t \mathbb{I}(y_i = 1) \; / \; t\)</span></li>
<li>By the <strong>Theorem 1.</strong>, we need restriction of the solution</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp; \Omega_1 = \{ (\mathbf{w}, a, b) \; : \; \lvert \mathbf{w} \rVert \leq R, \; |a| \leq R \kappa, \; |b| \leq R \kappa \} \\[10pt]
&amp; \Omega_2 = \{ \alpha \; : \; |\alpha| \leq 2R \kappa \} \\[10pt]
&amp; \kappa = \sup_{x \in \mathcal{X}} \; \lVert x \rVert &lt; \inf
\end{split}
\end{equation}\]</span>
<br>
<span class="math display">\[\begin{equation}
\hat{F}_t (v, \alpha, z) = 
\begin{cases}
(1 - \hat{p}_t) (\mathbf{w}^{\text{T}} x - a)^2 - 2 (1 + \alpha) (1 - \hat{p}_t) \mathbf{w}^{\text{T}} x - \hat{p}_t (1 - \hat{p}_t) \alpha^2 &amp; \text{if} \quad y = 1 \\[10pt]
\hat{p}_t (\mathbf{w}^{\text{T}} x - b)^2 + 2 (1 + \alpha) \hat{p}_t \mathbf{w}^{\text{T}} x - \hat{p}_t (1 - \hat{p}_t) \alpha^2 &amp; \text{if} \quad y = -1
\end{cases}
\end{equation}\]</span>
<br>
<span class="math display">\[\begin{equation}
\nabla_{\mathbf{w}} \hat{F}_t (v, \alpha, z) =
\begin{cases}
2 (1 - \hat{p}_t) (\mathbf{w}^{\text{T}} x - a - 1 - \alpha) x &amp; \text{if} \quad y = 1 \\[10pt]
2 \hat{p}_t (\mathbf{w}^{\text{T}} x - b + 1 + \alpha) x &amp; \text{if} \quad y = -1
\end{cases}
\end{equation}\]</span>
<br>
<span class="math display">\[\begin{equation}
\nabla_{a} \hat{F}_t (v, \alpha, z) =
\begin{cases}
-2 (1 - \hat{p}_t) (\mathbf{w}^{\text{T}} x - a) &amp; \text{if} \quad y = 1 \\[10pt]
0 &amp; \text{if} \quad y = -1
\end{cases}
\end{equation}\]</span>
<br>
<span class="math display">\[\begin{equation}
\nabla_{b} \hat{F}_t (v, \alpha, z) =
\begin{cases}
0 &amp; \text{if} \quad y = 1 \\[10pt]
-2 \hat{p}_t (\mathbf{w}^{\text{T}} x - b) &amp; \text{if} \quad y = -1
\end{cases}
\end{equation}\]</span>
<br>
<span class="math display">\[\begin{equation}
\nabla_{\alpha} \hat{F}_t (v, \alpha, z) =
\begin{cases}
- 2 (1 - \hat{p}_t) (\hat{p}_t \alpha + \mathbf{w}^{\text{T}} x) &amp; \text{if} \quad y = 1 \\[10pt]
2 \hat{p}_t (\hat{p}_t \alpha + \mathbf{w}^{\text{T}} x - \alpha) &amp; \text{if} \quad y = -1
\end{cases}
\end{equation}\]</span></p>
<p><br>
<img src="images/Table_01.PNG" />
<br></p>
<ul>
<li>In step 6,</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp; \bar{\gamma}_1 = \bar{\gamma}_0 + \gamma_1 = \gamma_1 \\[10pt]
&amp; \bar{\gamma}_2 = \bar{\gamma}_1 + \gamma_2 = \gamma_1 + \gamma_2 \\[10pt]
&amp; \bar{\gamma}_3 = \bar{\gamma}_2 + \gamma_3 = \gamma_1 + \gamma_2 + \gamma_3 \\[10pt]
&amp; \Rightarrow \bar{\gamma}_k = \sum_{j = 1}^k \gamma_j
\end{split}
\end{equation}\]</span></p>
<ul>
<li>In step 7, 8,</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{split}
&amp; \bar{v}_1 = \frac{1}{\bar{\gamma}_1} (\bar{\gamma}_0 \bar{v}_0 + \gamma_1 v_1) = v_1 \\[10pt]
&amp; \bar{v}_2 = \frac{1}{\bar{\gamma}_2} (\bar{\gamma}_1 \bar{v}_1 + \gamma_2 v_2) = \frac{\gamma_1 v_1 + \gamma_2 v_2}{\gamma_1 + \gamma_2} \\[10pt]
&amp; \bar{v}_3 = \frac{1}{\bar{\gamma}_3} (\bar{\gamma}_2 \bar{v}_2 + \gamma_2 v_2) = \frac{\gamma_1 v_1 + \gamma_2 v_2 + \gamma_3 v_3}{\gamma_1 + \gamma_2 + \gamma_3} \\[10pt]
&amp; \Rightarrow \bar{v}_k = \frac{\sum_{j = 1}^k \gamma_j v_j}{\sum_{j = 1}^k \gamma_j} \\[10pt]
&amp; \Rightarrow \bar{\alpha}_k = \frac{\sum_{j = 1}^k \gamma_j \alpha_j}{\sum_{j = 1}^k \gamma_j}
\end{split}
\end{equation}\]</span></p>
<hr />
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<ul>
<li>Let <span class="math inline">\(u = (v, \alpha) = (\mathbf{w}, a, b, \alpha)\)</span></li>
<li>The quality of an approximation solution <span class="math inline">\((\bar{v}_t, \bar{\alpha}_t)\)</span> to the SPP problem is measure by the duality gap</li>
</ul>
<p><span class="math display">\[\begin{equation}
\epsilon_f (\bar{v}_t \bar{\alpha}_t) = \max_{\alpha \in \Omega_2} f(\bar{v}_t, \alpha) - \min_{v \in \Omega_1} f(v, \bar{\alpha}_t)
\end{equation}\]</span></p>
<ul>
<li><strong>Theorem 2.</strong> Assume that samples <span class="math inline">\(\{ (x_1, y_1), \cdots, (x_T, y_T) \}\)</span> are i.i.d. drawn from a distribution <span class="math inline">\(\rho\)</span> over <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span>, let <span class="math inline">\(\Omega_1\)</span>, <span class="math inline">\(\Omega_2\)</span> be given by the former section and the step size given by <span class="math inline">\(\{ \gamma_t &gt; 0 : t \in \mathbb{N} \}\)</span>. For sequence <span class="math inline">\(\{ (\bar{v}_t, \bar{\alpha}_t) : t \in [1,T] \}\)</span> generated by <em>SOLAM</em>, and any <span class="math inline">\(0 &lt; \delta &lt;1\)</span>, with probability <span class="math inline">\(1 - \delta\)</span>, the following holds where <span class="math inline">\(C_{\kappa}\)</span> is an absolute constant independent of <span class="math inline">\(R\)</span> and <span class="math inline">\(T\)</span></li>
</ul>
<p><span class="math display">\[\begin{equation}
\epsilon_f (\bar{v}_t \bar{\alpha}_t) \leq C_{\kappa} \max(R^2, 1) \sqrt{\log{\frac{4T}{\delta}}} (\sum_{j = 1}^T \gamma_j)^{-1} \left( 1 + \sum_{j = 1}^T \gamma_j^2 + (\sum_{j = 1}^T \gamma_j^2)^{1/2} + \sum_{j = 1}^T \frac{\gamma_j}{\sqrt{j}} \right)
\end{equation}\]</span></p>
<ul>
<li><strong>Corollary 1.</strong> Under the same assumptions as in <strong>Theorem 2.</strong>, and <span class="math inline">\(\{ \gamma_j = \zeta j^{-\frac{1}{2}} : j \in \mathbb{N} \}\)</span> with constant <span class="math inline">\(\zeta &gt; 0\)</span>, with probability <span class="math inline">\(1 - \delta\)</span> it holds the following</li>
</ul>
<p><span class="math display">\[\begin{equation}
|f(\bar{v}_T, \bar{\alpha}_T) - f^{\star}| \leq \epsilon_f (\bar{u}_T) = \mathcal{O} \left( \frac{\log{T} \sqrt{\log{(4T / \delta)}}}{\sqrt{T}} \right)
\end{equation}\]</span></p>
<hr />
</div>
<div id="experiments" class="section level2">
<h2>Experiments</h2>
<ul>
<li>5-fold cross validation to determine <span class="math inline">\(\zeta \in [1:9:100]\)</span>, and <span class="math inline">\(R \in 10^{[-1:1:5]}\)</span> by grid search</li>
<li>Averaging results from 5 runs of 5-fold cross validation</li>
<li>Multi-class dataset is transformed into binary classification by randomly partitioning the data into two groups, where each group includes the same number of classes</li>
<li>Comparison
<ul>
<li>OPAUC (Gao et al., 2013)</li>
<li>OAM (Zhao et al., 2011)</li>
<li>online Uni-Exp: weighted univariate exponential loss (Kotlowski et al., 2011)</li>
<li>B-SVM-OR: batch learning algorithm using the hinge loss surrogate of AUC (Joachims, 2006)</li>
<li>B-LS-SVM: batch learning algorithm using <span class="math inline">\(l_2\)</span> loss surrogate of AUC
<br>
<img src="images/Table_02.PNG" />
<br>
<img src="images/Table_03.PNG" />
<br>
<img src="images/Fig_01.PNG" />
<br></li>
</ul></li>
</ul>
<hr />
</div>
<div id="further-study" class="section level2">
<h2>Further Study</h2>
<ul>
<li>Joachims, 2005, <strong>A support vector method for multivariate performance measures</strong>, <em>International Conference on Machine Learning</em>. <a href="https://icml.cc/imls/conferences/2005/proceedings/papers/048_ASupport_Joachims.pdf">pdf</a></li>
<li>Kotlowski et al., 2011, <strong>Bipartite ranking through minimization of univariate loss</strong>, <em>International Conference on Machine Learning</em>. <a href="https://icml.cc/2011/papers/567_icmlpaper.pdf">pdf</a></li>
<li>Rakhlin et al., 2012, <strong>Making gradient descent optimal for strongly convex
stochastic optimization</strong>, <em>International Conference on Machine Learning</em>. <a href="https://icml.cc/Conferences/2012/papers/261.pdf">pdf</a></li>
<li>Ying and Zhou, 2016, <strong>Online pairwise learning algorithms</strong>, <em>Neural Computation</em>. <a href="https://arxiv.org/pdf/1502.07229.pdf">pdf</a></li>
</ul>
<hr />
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li>Gao et al., 2013, <strong>One-pass AUC optimization</strong>, <em>International Conference on Machine Learning</em>. <a href="http://proceedings.mlr.press/v28/gao13.pdf">pdf</a>, <a href="https://haaforever.github.io/blog/post/2020/12/07/one-pass-auc-optimization/">summary</a></li>
<li>Joachims, 2006, <strong>Training linear svms in linear time</strong>, <em>International Conference on Knowledge Discovery and Data Mining</em>. <a href="http://web.engr.oregonstate.edu/~huanlian/teaching/machine-learning/2017fall/extra/linear-svm-linear-time.pdf">pdf</a></li>
<li>Kotlowski et al., 2011, <strong>Bipartite ranking through minimization of univariate loss</strong>, <em>International Conference on Machine Learning</em>. <a href="https://icml.cc/2011/papers/567_icmlpaper.pdf">pdf</a></li>
<li>Zhao et al., 2011, <strong>Online AUC maximization</strong>, <em>International Conference on Machine Learning</em>. <a href="http://www.icml-2011.org/papers/198_icmlpaper.pdf">pdf</a>, <a href="https://haaforever.github.io/blog/post/2020/12/07/online-auc-maximization/">summary</a></li>
</ul>
</div>
